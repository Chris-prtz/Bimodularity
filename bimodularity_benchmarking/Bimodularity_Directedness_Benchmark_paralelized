import cupy as cp
from cuml.cluster import KMeans as cuKMeans
from cuml.metrics import silhouette_score as cu_silhouette_score

# Some directed modularity toolinss
def configuration_null_paralelized(a_mat: cp.ndarray, null_model: str = "outin"):
    k_in = a_mat.sum(axis=0).reshape((1, -1))
    k_out = a_mat.sum(axis=1).reshape((1, -1))

    if null_model == "in":
        z = k_in.T @ k_in
    elif null_model == "out":
        z = k_out.T @ k_out
    elif null_model == "inout":
        z = k_in.T @ k_out
    elif null_model == "outin":
        z = k_out.T @ k_in
    elif null_model == "avg":
        z = (k_in.T @ k_in + k_out.T @ k_out) / 2
    elif null_model == "send":
        z = send_receive_probability_paralelized(a_mat)[0]
    elif null_model == "receive":
        z = send_receive_probability_paralelized(a_mat)[1]
    return z / a_mat.sum()


def send_receive_probability_paralelized(adj:  cp.ndarray):
    n_edges = adj.sum()

    in_deg = cp.atleast_2d(adj.sum(axis=0))
    out_deg = cp.atleast_2d(adj.sum(axis=1))

    in_deg_squared = (in_deg**2).sum()
    out_deg_squared = (out_deg**2).sum()

    send_prob = cp.outer(out_deg, out_deg) * in_deg_squared / n_edges**2
    receive_prob = cp.outer(in_deg, in_deg) * out_deg_squared / n_edges**2

    return send_prob, receive_prob


def modularity_matrix_paralelized(a_mat: cp.ndarray, null_model: str = "outin"):

    z = configuration_null_paralelized(a_mat, null_model)

    return a_mat - z

def sorted_SVD_parallelized(matrix: cp.ndarray, fix_negative: bool = False, sort_by_q: bool = False):

    U, S, Vh = cp.linalg.svd(matrix, full_matrices=True)

    if fix_negative:
        for i, _ in enumerate(matrix):
            if U[:, i].T @ Vh[i] < 0:
                Vh[i] *= -1
                S[i] *= -1

    sort_id = cp.flip(cp.argsort(S))
    if sort_by_q:
        q_s = S * cp.diag(Vh @ U)
        sort_id = cp.flip(cp.argsort(q_s))

    S = S[sort_id]
    U = U[:, sort_id]
    Vh = Vh[sort_id]

    return U, S, Vh

def adjacency_matrix_directedness_transform_paralellized(adj_matrix, gamma):
    """
    Applies an element-wise directional strength transformation to an adjacency matrix.
    
    This function implements the formula:
    result[i,j] = 2 * A[i,j] * (A[i,j])^γ / ((A[i,j])^γ + (A[j,i])^γ)
    
    Each element is modified based only on its own value and the value of its 
    transpose counterpart, creating asymmetric directional emphasis. Higher gamma 
    values amplify existing edge strengths relative to their reverse directions.
    
    Parameters:
    -----------
    adj_matrix : numpy.ndarray
        Icput adjacency matrix representing directed graph connections
    gamma : float
        Exponent parameter controlling directional emphasis (γ > 0)
        - γ = 1: Linear weighting based on forward/reverse edge ratio
        - γ > 1: Exponential amplification of stronger directions
        - γ < 1: Smoothing effect, reducing extreme directional differences
    
    Returns:
    --------
    numpy.ndarray
        Modified adjacency matrix where each element result[i,j] depends only on 
        the original elements A[i,j] and A[j,i]. All operations are element-wise.
        
    """
    A = cp.array(adj_matrix) ** gamma
    div = A + A.T  # Sum of the element and its transpose
    result = cp.divide(A, div, out= cp.zeros_like(A), where=(A + A.T) != 0)  # Avoid division by zero

    return result


def edge_bicommunities_paralelized(
    adjacency,
    U,
    V,
    n_components,
    method="partition",
    n_kmeans=10,
    verbose=False,
    scale_S=None,
    assign_only=False,
    **kwargs,
) -> tuple:

    n_nodes = adjacency.shape[0]

    if scale_S is None:
        scale_S = cp.ones(n_components)

    # u_features = U[:, :n_components] * cp.sqrt(scale_S)
    # v_features = V[:, :n_components] * cp.sqrt(scale_S)

    u_features = U[:, :n_components] * scale_S
    v_features = V[:, :n_components] * scale_S

    if method in ["partition", "sign"]:
        u_features = cp.sign(u_features).astype(int)
        v_features = cp.sign(v_features).astype(int)

    edge_out = cp.array([u_features] * n_nodes).T
    edge_in = cp.array([v_features] * n_nodes)
    edge_in = cp.moveaxis(edge_in, -1, 0)

    # edge-based clustering
    edge_assignments = cp.concatenate([edge_out, edge_in], axis=0)
    edge_assignments_vec = edge_assignments.reshape((2 * n_components, -1)).T

    edge_assignments_vec = edge_assignments_vec[(adjacency != 0).reshape(-1)]

    if assign_only:
        return edge_assignments_vec

    if method in ["partition", "sign"]:
        clusters = cp.unique(edge_assignments_vec, axis=0)
        n_clusters = clusters.shape[0]
        if verbose:
            print(f"Found {n_clusters} clusters !")

        cluster2num = {tuple(c): i + 1 for i, c in enumerate(clusters)}

        edge_clusters = cp.array([cluster2num[tuple(c)] for c in edge_assignments_vec])

    elif method == "kmeans":
        if n_kmeans is None:
            n_kmeans = get_best_k(edge_assignments_vec, verbose=verbose, **kwargs)

        kmeans = cuKMeans(n_clusters=n_kmeans, random_state=0, n_init="auto").fit(
            edge_assignments_vec
        )
        edge_clusters = kmeans.labels_ + 1

        n_clusters = edge_clusters.max()
        if verbose:
            print(f"Found {n_clusters} clusters !")
    else:
        raise ValueError(
            "Method not recognized (possible values: partition, sign, kmeans)"
        )

    edge_clusters_mat = cp.zeros((n_nodes, n_nodes), dtype=int)
    edge_clusters_mat[adjacency != 0] = edge_clusters

    return edge_clusters, edge_clusters_mat


def get_best_k(X, max_k=10, verbose=False):
    print(f"Running silhouette analysis for k = 2 to {max_k} ...")
    n_clusters = cp.arange(2, max_k)
    silhouette = cp.zeros(n_clusters.shape[0])

    for i, n in enumerate(n_clusters):
        kmeans = cuKMeans(n_clusters=n, random_state=0, n_init="auto").fit(X)
        silhouette[i] = cu_silhouette_score(X, kmeans.labels_)

        if verbose:
            print(f"Silhouette score for K={n} is : {silhouette[i]:1.2f}")

    print(
        f"Best average silhouette_score is : {cp.max(silhouette):1.2f} for K={n_clusters[cp.argmax(silhouette)]}"
    )
    return n_clusters[cp.argmax(silhouette)]


def get_node_clusters(edge_clusters, edge_clusters_mat, method="bimod", scale=True):
    n_nodes = edge_clusters_mat.shape[0]
    n_clusters = cp.max(edge_clusters)

    if method == "probability":
        # Aggregate edges to nodes using cluster probability (number of edges)
        n_per_cluster = cp.zeros((n_nodes, n_clusters))
        for cluster_id in cp.arange(1, cp.max(edge_clusters_mat) + 1):
            n_per_cluster[:, cluster_id - 1] = cp.sum(
                edge_clusters_mat == cluster_id, axis=1
            )
            n_per_cluster[:, cluster_id - 1] += cp.sum(
                edge_clusters_mat == cluster_id, axis=1
            )

        cluster_prob = n_per_cluster / n_per_cluster.sum(axis=1)[:, None]

        cluster_maxprob = cp.argmax(cluster_prob, axis=1) + 1

        return cluster_maxprob, cluster_prob
    if "bimod" in method:
        sending_communities = cp.zeros((n_clusters, n_nodes))
        receiving_communities = cp.zeros((n_clusters, n_nodes))

        for cluster_id in cp.arange(1, cp.max(edge_clusters_mat) + 1):
            sending_communities[cluster_id - 1] = cp.sum(
                edge_clusters_mat == cluster_id, axis=1
            )
            receiving_communities[cluster_id - 1] = cp.sum(
                edge_clusters_mat == cluster_id, axis=0
            )

        if scale:
            sending_communities = cp.nan_to_num(
                sending_communities / cp.sum(edge_clusters_mat > 0, axis=1),
                posinf=0,
                neginf=0,
            )
            receiving_communities = cp.nan_to_num(
                receiving_communities / cp.sum(edge_clusters_mat > 0, axis=0),
                posinf=0,
                neginf=0,
            )

        return sending_communities, receiving_communities


def bimod_index_nodes(adjacency, send_com, receive_com, scale=False):
    n_clusters = len(send_com)

    null_model = configuration_null_paralelized(adjacency, null_model="outin")

    bimod_indices = cp.zeros(n_clusters)

    for cluster_id in cp.arange(n_clusters):
        send_fltr = send_com[cluster_id] > 0
        receive_fltr = receive_com[cluster_id] > 0

        adj_contrib = adjacency[send_fltr][:, receive_fltr]
        null_contrib = null_model[send_fltr][:, receive_fltr]

        bimod_indices[cluster_id] = cp.sum(adj_contrib - null_contrib)

        if scale:
            all_edges = cp.sum(cp.atleast_2d(send_fltr).T @ cp.atleast_2d(receive_fltr))
            bimod_indices[cluster_id] /= all_edges
        # / cp.sum(adj_contrib > 0)

    return bimod_indices

def  get_communities_parallelized(A_sym, A_assym, alphas, gammas, n_kmeans, vector_id_max): 
    A_sym = cp.array(A_sym) if not isinstance(A_sym, cp.ndarray) else A_sym
    A_assym = cp.array(A_assym) if not isinstance(A_assym, cp.ndarray) else A_assym

    communities = []
    for gamma in gammas:
        g_com = []
        for alpha in alphas:
            model =  adjacency_matrix_directedness_transform_paralellized(A_sym + alpha*A_assym, gamma = gamma)

            # determine bicomunities 
            U, S, Vh = sorted_SVD_parallelized(modularity_matrix_paralelized(model, null_model="outin"))
            V = Vh.T

            edge_clusters, edge_clusters_mat = edge_bicommunities_paralelized(model, U, V, vector_id_max, method="kmeans",
                                                                    n_kmeans=n_kmeans, scale_S= S[:vector_id_max], verbose=False)

            sending_communities, receiving_communities = get_node_clusters(edge_clusters, edge_clusters_mat, method="bimodularity")  
            g_com.append((sending_communities, receiving_communities))
        communities.append(g_com)
    return communities